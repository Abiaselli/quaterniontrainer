# Quaternion concept model trainer
Quaternion-based model trainer

NOTE: The tokenizer generated by zipf tokenizer will need to have the word "vocab" and the first and last brackets removed to make it compatible with the trainer, not worth hunting down the issue when it only needs to be fixed once

Hello! My name is Austin, thanks for visiting this repo. This is a transformer model cnoncept based on quaternions and quaternion math, with the intention of creating a new model type which can simulate word relationships with quaternion vectors.

The general idea, is that the embedding dimension encodes a different property for each word in the vocabulary with a quaternion vector. I plan at some point to develop a better method for aligning information to different embedding indices and coding the AI to specifically compare different information or be initialized with certain unchanging values, but for now it is entirely generated through machine learning. 

It generally follows the ordinary method for transformer attention, and the usual concepts using quaternion dot product and multiplication, but the rotational position encoding is specific to quaternion math using quaternion rotation, with the intention of allowing the information to be mapped in a hypersphere sense to allow for abstract relationsips between tokens.

I will probably update the feed forward design at some point to include quaternion specific operations, or create a wrapper that does this, but for now I wanted to get something the worked. I tested it on a small test dataset, and it converged for small models, but I am currently working to see if it can generalize to larger datasets as that one was small enough to allow 500 epochs and overfitting.

Included is a program that generates a tokenizer based on Zipf's law, where you can load a corpus dataset and have it iterate over it and generate a tokenizer vocab following Zipf's law where the lowest number is used twice as much as its next highest number (excluding special tokens,) so that the most often used words have lower indices. This is a useful optimization for any model. I have it currently limited to 30k tokens and a 25 character word limit.

Also included is a folder with the latest version of my test model. It is set to 8 layers, embed size of 8 (8x4 because of the quaternion vectors) and a vocab size of 30001 (it should be 30k, but it works so that issue is low priority.) The parameter count is an approximation, around 500k for this model at 7MB. This is calculated by num_layers * (4 * (hidden_size ** 2) + 2 * embed_size * hidden_size), but this was suggested by ChatGPT and doesn't include vocab size or attention so it depends on how you count parameters. Every layer should contain 2 feed forward layers, hidden size and embed dim are the same because multihead attention is not used with this, so we have a norm of embedx4 (for quaternion) times 2 feedforward layers of hiddenxhidden, and the norm is done twice while each input passed is added with the input (and attention.)

If you use the test model, a batch size of 8 will take up about 8-9 GB on GPU (including the model.) It has reached a general loss of 4-6 with any input after about 3 epochs with a range of data, but requires multipe epochs to align with a dataset I believe. I plan to try out a larger model at some point, or potentially increase to octonions or something similar.

Thanks, I hope you like it and am here to answer any questions!
