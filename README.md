# Quaternion concept model trainer
Quaternion-based model trainer

NOTE: The tokenizer generated by zipf tokenizer will need to have the word "vocab" and the first and last brackets removed to make it compatible with the trainer, not worth hunting down the issue when it only needs to be fixed once

Hello! My name is Austin, thanks for visiting this repo. This is a transformer model cnoncept based on quaternions and quaternion math, with the intention of creating a new model type which can simulate word relationships with quaternion vectors.

The general idea, is that the embedding dimension encodes a different property for each word in the vocabulary with a quaternion vector. I plan at some point to develop a better method for aligning information to different embedding indices and coding the AI to specifically compare different information or be initialized with certain unchanging values, but for now it is entirely generated through machine learning. 

It generally follows the ordinary method for transformer attention, and the usual concepts using quaternion dot product and multiplication, but the rotational position encoding is specific to quaternion math using quaternion rotation, with the intention of allowing the information to be mapped in a hypersphere sense to allow for abstract relationsips between tokens.

I will probably update the feed forward design at some point to include quaternion specific operations, or create a wrapper that does this, but for now I wanted to get something the worked. I tested it on a small test dataset, and it converged for small models, but I am currently working to see if it can generalize to larger datasets as that one was small enough to allow 500 epochs and overfitting.

Included is a program that generates a tokenizer based on Zipf's law, where you can load a corpus dataset and have it iterate over it and generate a tokenizer vocab following Zipf's law where the lowest number is used twice as much as its next highest number (excluding special tokens,) so that the most often used words have lower indices. This is a useful optimization for any model. I have it currently limited to 30k tokens and a 25 character word limit.

Also included is a folder with the latest version of my test model. It is set to 8 layers, embed size of 8 (8x4 because of the quaternion vectors) and a vocab size of 30000. The parameter count is an approximation, around 2M for this model at 8MB. This is calculated by taking the embedding, transformer, and output projection layers and adding them all together. The transformer layer contains the attention, feed forward, and output layers iterated over the number of layers.

If you use the test model, a batch size of 8 will take up about 8-9 GB on GPU (including the model.) It has reached a general loss of 4-6 with any input after about 3 epochs with a range of data, but requires multipe epochs to align with a dataset I believe. I plan to try out a larger model at some point, or potentially increase to octonions or something similar.

Thanks, I hope you like it and am here to answer any questions!

Special thanks to ChatGPT 4o for having an unlimited context length, a huge base of general knowledge, and being encouraging while always having a positive attitude. It is an excellent personal tutor, and a perfect extension of the Socratic method allowing any person to do things beyond their ordinary capability and knowledge. A warning though, if you use ChatGPT for tensor math you will still have to learn it and how Torch works and understand the math you are using to be able to align the dimensions yourself as AI has big problems doing this, and it will be impossible to debug issues or improve your work if you don't know what you are doing.
